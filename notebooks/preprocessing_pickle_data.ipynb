{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:32:23.180381Z",
     "start_time": "2019-03-12T22:32:22.512505Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from os.path import isfile, join, exists\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import ALNSv2\n",
    "# from CompAnalysis.data_import import data_cordeau\n",
    "# from CompAnalysis.data_import import data_solomun\n",
    "\n",
    "\n",
    "base_path = \"C:\\\\Users\\manuf\\\\OneDrive\\\\Dokumente\\\\Universitaet\\\\Masterthesis\\\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:32:23.331576Z",
     "start_time": "2019-03-12T22:32:23.321497Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get slope and distance matrix\n",
    "def get_elevation_matrix(elevations):\n",
    "    \"\"\"\n",
    "    Get the elevation matrix for a list elevations (simple)\n",
    "\n",
    "    Annotation:\n",
    "    We report the elevation in kms so that it is inline with the km reporting of the distances\n",
    "\n",
    "    \"\"\"\n",
    "    nr_nodes = len(elevations)        \n",
    "    elevation_matrix = [[0 for i in range(nr_nodes)] for j in range(nr_nodes)]\n",
    "\n",
    "    for i in range(nr_nodes):\n",
    "        for j in range(i+1, nr_nodes):\n",
    "            # altitude difference to get from i to j\n",
    "            elevation_distance = (elevations[j] - elevations[i])/1000\n",
    "            elevation_matrix[i][j] = elevation_distance\n",
    "            elevation_matrix[j][i] = -elevation_distance # obviously the elevation is shifted now\n",
    "    return elevation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pirmin build cases (CASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\manuf\\\\OneDrive\\\\Dokumente\\\\Universitaet\\\\Masterthesis\\\\data\\\\vrpldtt_pirmin\\\\demandFinal_Clean.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e1791093a58f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msheet_nr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"demandFinal_Clean.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"customers ({sheet_nr})\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_86\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_86\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_86\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, **kwds)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     return io.parse(\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_86\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_86\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\manuf\\\\OneDrive\\\\Dokumente\\\\Universitaet\\\\Masterthesis\\\\data\\\\vrpldtt_pirmin\\\\demandFinal_Clean.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = join(base_path, \"vrpldtt_pirmin\")\n",
    "\n",
    "data = []\n",
    "for sheet_nr in range(1, 7):\n",
    "    data.append(pd.read_excel(join(path, \"demandFinal_Clean.xlsx\"), sheet_name = f\"customers ({sheet_nr})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goalis to build demand CSVs and combine all demand with location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_distance_data(location_data):\n",
    "    # fill the distance matrix information\n",
    "    for i in range(7, 28):\n",
    "        for j in range(i, 28):\n",
    "            location_data.iloc[j-7, i] = location_data.iloc[i-7,j] #switch\n",
    "            location_data.iloc[i-7, i] = 0 # diagonal\n",
    "        \n",
    "    return location_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city_id, distance_name in enumerate([\"matrix-fukuoka.xlsx\", \n",
    "                                         \"matrix-madrid.xlsx\", \n",
    "                                         \"matrix-pittsburgh.xlsx\", \n",
    "                                         \"matrix-seattle.xlsx\",\n",
    "                                        \"matrix-sydney.xlsx\"]):\n",
    "    for data_id, demand_data in enumerate(data):\n",
    "        distance_raw_name = distance_name[:-5]\n",
    "\n",
    "        location_data = pd.read_excel(join(path, distance_name),header=0)\n",
    "        location_data[\"demand\"] = demand_data[\"demand\"]\n",
    "        location_data[\"tw a\"] = demand_data[\"Twa\"]\n",
    "        location_data[\"tw b\"] = demand_data[\"TWb\"]\n",
    "        location_data[\"s\"] = demand_data[\"s\"]\n",
    "\n",
    "        del location_data[\"out of file\"]\n",
    "\n",
    "        # Prun the data to right size (20 customers max)\n",
    "        location_data = location_data.iloc[:21,:28]\n",
    "        location_data = fill_distance_data(location_data)\n",
    "\n",
    "        # export data\n",
    "        file_name = f\"C{(city_id+1)*100+data_id+1}.csv\"\n",
    "        location_data.to_csv(join(path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pirmin Parse results (BKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(join(\"C:\\\\Users\\\\manuf\\\\OneDrive\\\\Dokumente\\\\Universitaet\\\\Masterthesis\\\\archive\\\\resultsFinal_Clean.xlsx\"))\n",
    "\n",
    "data[\"case\"] = data[\"map\"] + (data[\"demand ID\"]).apply(str)\n",
    "\n",
    "for index in data.index:\n",
    "    with open(join(\"C:\\\\Users\\\\manuf\\\\OneDrive\\\\Dokumente\\\\Universitaet\\\\Masterthesis\\\\data\", f\"{data.loc[index, 'case']}.txt\"), \"w\") as file:\n",
    "        file.write(str(data.loc[index, 'obj']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pirmin Build data obj (DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:32:28.860824Z",
     "start_time": "2019-03-12T22:32:28.845865Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the main macro to read the base data and build a valid data object\n",
    "\n",
    "\"\"\"\n",
    "from os.path import isfile, join, exists\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from ALNSv2 import ALNSData\n",
    "\n",
    "def build_data_object(path,\n",
    "                      nr_vehicles=None,\n",
    "                      nr_load_buckets=10,\n",
    "                      weight_interval_size=-1,\n",
    "                      vehicle_capacity = 150,\n",
    "                      vehicle_weight=140):\n",
    "    \n",
    "    # Get slope and distance matrix\n",
    "    def get_elevation_matrix(elevations):\n",
    "        \"\"\"\n",
    "        Get the elevation matrix for a list elevations (simple)\n",
    "\n",
    "        Annotation:\n",
    "        We report the elevation in kms so that it is inline with the km reporting of the distances\n",
    "\n",
    "        \"\"\"\n",
    "        nr_nodes = len(elevations)        \n",
    "        elevation_matrix = [[0 for i in range(nr_nodes)] for j in range(nr_nodes)]\n",
    "\n",
    "        for i in range(nr_nodes):\n",
    "            for j in range(i+1, nr_nodes):\n",
    "                # altitude difference to get from i to j\n",
    "                elevation_distance = (elevations[j] - elevations[i])\n",
    "                elevation_matrix[i][j] = elevation_distance\n",
    "                elevation_matrix[j][i] = -elevation_distance # obviously the elevation is shifted now\n",
    "        return elevation_matrix\n",
    "\n",
    "    \n",
    "    # Some basic input checks\n",
    "    data = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    nr_nodes = data.shape[0]\n",
    "    nr_depots = 1\n",
    "    nr_customers = nr_nodes-nr_depots\n",
    "    weight_interval_size = weight_interval_size\n",
    "    vehicle_weight = vehicle_weight\n",
    "    vehicle_capacity =vehicle_capacity\n",
    "\n",
    "    # set standard settings\n",
    "    if nr_vehicles is None:\n",
    "        nr_vehicles = nr_customers\n",
    "    else:\n",
    "        nr_vehicles = nr_vehicles\n",
    "\n",
    "\n",
    "    distance_matrix = data.iloc[:, 7:].values\n",
    "    logging.info(\"Distance matrix successfully retrieved\")\n",
    "\n",
    "    # get network information\n",
    "    altitude = data[\"elevation\"].values\n",
    "    elevation_matrix = get_elevation_matrix(altitude)\n",
    "\n",
    "    logging.info(\"Elevation matrix successfully built\")\n",
    "\n",
    "    # get process information\n",
    "    demand_array = data[\"demand\"].values[nr_depots:]\n",
    "    window_start = data[\"tw a\"].values[nr_depots:]\n",
    "    window_end = data[\"tw b\"].values[nr_depots:]\n",
    "    service_times = data[\"s\"].values[nr_depots:]\n",
    "\n",
    "    # Build the data object.  It performs internal preprocessing!\n",
    "    logging.info(\"General info retrieved. Preprocessing started.\")\n",
    "\n",
    "    data_object = ALNSData(nr_veh=nr_vehicles,\n",
    "                           nr_nodes=nr_nodes,\n",
    "                           nr_customers=nr_customers,\n",
    "                           demand=demand_array,\n",
    "                           service_times=service_times,\n",
    "                           start_window=window_start,\n",
    "                           end_window=window_end,\n",
    "                           elevation_m=elevation_matrix,\n",
    "                           distance_m=distance_matrix,\n",
    "                           load_bucket_size=weight_interval_size,\n",
    "                           nr_load_buckets=nr_load_buckets,\n",
    "                           vehicle_weight=vehicle_weight,\n",
    "                           vehicle_capacity=vehicle_capacity)\n",
    "\n",
    "    logging.info(\"Preprocessing done. Data object built\")\n",
    "    return data_object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build data object Solomon | Gehring & Homberger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ALNSv2 import ALNSData\n",
    "\n",
    "def build_data_object(file_path):\n",
    "    \"\"\"\n",
    "    Build the data object from a text file\n",
    "    with syntax introduced by solomun\n",
    "\n",
    "    http://neo.lcc.uma.es/vrp/vrp-instances/capacitated-vrp-with-time-windows-instances/\n",
    "    \"\"\"\n",
    "    # 1) Parse data\n",
    "    row_id = 0\n",
    "\n",
    "    coordinates = [] #\n",
    "    number_vehicles = 0 #\n",
    "    nr_nodes = 0 #\n",
    "    nr_customers = 0 #\n",
    "    demand_array = [] #\n",
    "    service_times = [] #\n",
    "    window_start = [] #\n",
    "    window_end = [] #\n",
    "    vehicle_capacity = 0 #\n",
    "\n",
    "    # Get base data from file\n",
    "    with open(file_path, 'r') as file:\n",
    "        for row in file:\n",
    "            if row_id == 0:\n",
    "                instance_id = row\n",
    "            \n",
    "            if row_id == 4:\n",
    "                row_data = row.replace(\"         \", \",\").strip().split(\",\")\n",
    "                number_vehicles, vehicle_capacity = int(row_data[0]), int(row_data[1])\n",
    "            \n",
    "            # start of node data\n",
    "            if row_id > 8:\n",
    "                # quick and dirty\n",
    "                row_data = [x for x in row.replace(\" \", \",\").split(\",\") if (x != \"\") and (x != \"\\n\")]\n",
    "                if not row_data:\n",
    "                    break\n",
    "\n",
    "                coordinates.append((int(row_data[1]), int(row_data[2])))\n",
    "                \n",
    "                if row_id > 9:\n",
    "                    # start of customer data\n",
    "                    demand_array.append(int(row_data[3]))\n",
    "                    window_start.append(int(row_data[4]))\n",
    "                    window_end.append(int(row_data[5]))\n",
    "                    service_times.append(int(row_data[6]))\n",
    "        \n",
    "            nr_customers = len(demand_array)\n",
    "            nr_nodes = nr_customers + 1\n",
    "            elevations = [0]*nr_nodes\n",
    "            row_id += 1\n",
    "        \n",
    "    time_cube = [get_distance_matrix(coordinates)] # We must give it a new artifical dimension\n",
    "\n",
    "    # 2) Build data object\n",
    "    data_object = ALNSData(nr_veh=number_vehicles,\n",
    "                       nr_nodes=nr_nodes,\n",
    "                       nr_customers=nr_customers,\n",
    "                       demand=demand_array,\n",
    "                       service_times=service_times,\n",
    "                       start_window=window_start,\n",
    "                       end_window=window_end,\n",
    "                       time_c=time_cube,\n",
    "                       vehicle_capacity=vehicle_capacity)\n",
    "    return data_object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build cordeau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(row):\n",
    "    return [float(x) for x in row.replace(\" \", \",\").replace(\"\\n\",\"\").split(\",\") if x != \"\"]\n",
    "\n",
    "def build_data_object(file_path):\n",
    "    \"\"\"\n",
    "    Parse data based on the cordeau syntax\n",
    "    http://neo.lcc.uma.es/vrp/vrp-instances/description-for-files-of-cordeaus-instances/\n",
    "    \n",
    "    Annotation:\n",
    "    Cordeau passes some irrelevant information because his templates \n",
    "    are applicatble for a wide range of problem types (e.g. PVRP, VRPTW, etc.)\n",
    "    We just ignore some things (e.g. maximum route duration)\n",
    "    \"\"\"\n",
    "    coordinates = []\n",
    "    service_times = []\n",
    "    demand_array = []\n",
    "    window_start = []\n",
    "    window_end = []\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        for row_id, row in enumerate(file):\n",
    "            p_row = parse_row(row)\n",
    "            # general data row\n",
    "            if row_id == 0:\n",
    "                p_type, number_vehicles, nr_customers, nr_days = p_row\n",
    "\n",
    "            # Route limits\n",
    "            if row_id == 1:\n",
    "                route_max_duration, vehicle_capacity = p_row\n",
    "                \n",
    "                if route_max_duration > 0:\n",
    "                    raise ValueError(\"Max duration not considered in the current ALNS model\")\n",
    "                \n",
    "            # Depot information\n",
    "            if row_id > 1:\n",
    "                coordinates.append((p_row[1], p_row[2]))\n",
    "                                  \n",
    "                # customer information\n",
    "                if row_id > 2:\n",
    "                    if row != \"\" and row != \"\\n\":\n",
    "                        service_times.append(p_row[3])\n",
    "                        demand_array.append(p_row[4])\n",
    "                        window_start.append(p_row[-2])\n",
    "                        window_end.append(p_row[-1])\n",
    "          \n",
    "    time_cube = [get_distance_matrix(coordinates)] # We must give it a new artifical dimension\n",
    "\n",
    "    data_object = ALNSData(nr_veh=int(number_vehicles),\n",
    "                    nr_nodes=int(nr_customers+1),\n",
    "                    nr_customers=int(nr_customers),\n",
    "                    demand=demand_array,\n",
    "                    service_times=service_times,\n",
    "                    start_window=window_start,\n",
    "                    end_window=window_end,\n",
    "                    time_c=time_cube,\n",
    "                    vehicle_capacity=int(vehicle_capacity))\n",
    "    return data_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:32:35.179831Z",
     "start_time": "2019-03-12T22:32:35.166961Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_data(python_callable, directory, extention = \"\"):\n",
    "    \"\"\"\n",
    "    Utility function to perform preprocessing on all case files\n",
    "    in a given directory\n",
    "    \n",
    "    Accepts subfolders in \"cases\" and builds the same subfolder structure\n",
    "    in the \"data\" subdirectory\n",
    "    \n",
    "    @param python_callable: Function object that takes a file path\n",
    "                            and returns a data object. Defines the\n",
    "                            preprocessing routine\n",
    "                            \n",
    "    @param directory:       Directory on which the preprocessing is\n",
    "                            performed. Must have a \"data\" and \"cases\"\n",
    "                            subdirectory\n",
    "    \"\"\"\n",
    "    def get_object(directory):\n",
    "        \"\"\"\n",
    "        Generator object to return the file paths\n",
    "        Use generator to avoid memory issues with large directories\n",
    "        \"\"\"\n",
    "        for f in listdir(directory):\n",
    "            if isfile(join(directory,f)):\n",
    "                yield \"file\", f\n",
    "            else:\n",
    "                yield \"subdir\", f\n",
    "    \n",
    "    cases_path = join(directory, \"cases\", extention)\n",
    "    data_path = join(directory, \"data\", extention)\n",
    "    \n",
    "    sub_dirs = []\n",
    "    \n",
    "    # Check all files in a directory\n",
    "    # If there is a subdirectory iterate over it recursevly as well\n",
    "    for f_type, f in get_object(cases_path):\n",
    "        # 1) Perform preprocessing on all file objects\n",
    "        if f_type == \"file\":\n",
    "            data_object = python_callable(join(cases_path, f))\n",
    "\n",
    "            file_name_raw = f.split(\".\")[0]\n",
    "            with open(join(data_path, f\"{file_name_raw}.pkl\"), \"wb\") as file:\n",
    "                pickle.dump(data_object, file)\n",
    "        else:\n",
    "            sub_dirs += [f]\n",
    "    \n",
    "    # 2) Recursively iterate over all subdirectories and\n",
    "    # perform the preprocessing on all files in them as well\n",
    "    while sub_dirs:\n",
    "        print(sub_dirs)\n",
    "        if sub_dirs:\n",
    "            # build subdir to enable writing into it\n",
    "            new_subdir = sub_dirs.pop()\n",
    "            if not exists(join(data_path, new_subdir)):\n",
    "                makedirs(join(data_path, new_subdir))\n",
    "            build_data(python_callable, directory, join(extention,new_subdir))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_data(build_data_object, join(base_path, \"vrptw_cordeau\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solomon_100', 'solomon_25', 'solomon_50']\n",
      "['solomon_100', 'solomon_25']\n",
      "['solomon_100']\n"
     ]
    }
   ],
   "source": [
    "build_data(build_data_object, join(base_path, \"vrptw_solomon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:32:42.649049Z",
     "start_time": "2019-03-12T22:32:38.325178Z"
    }
   },
   "outputs": [],
   "source": [
    "build_data(build_data_object, join(base_path, \"vrpldtt_fontaine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:40:50.318638Z",
     "start_time": "2019-03-12T22:37:57.090058Z"
    }
   },
   "outputs": [],
   "source": [
    "build_data(build_data_object, join(base_path, \"vrpldtt_freytag\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homberger_1000', 'homberger_200', 'homberger_400', 'homberger_600', 'homberger_800']\n",
      "['homberger_1000', 'homberger_200', 'homberger_400', 'homberger_600']\n",
      "['homberger_1000', 'homberger_200', 'homberger_400']\n",
      "['homberger_1000', 'homberger_200']\n",
      "['homberger_1000']\n"
     ]
    }
   ],
   "source": [
    "build_data(build_data_object, join(base_path, \"vrptw_gehring_homberger\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle object parts (distance / time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T18:41:11.756962Z",
     "start_time": "2019-03-12T18:41:11.736960Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\manuf\\\\OneDrive\\\\Dokumente\\\\Universitaet\\\\Masterthesis\\\\data\\\\vrpldtt_fontaine\\\\data\"\n",
    "sol1 = pickle.load(open(join(data_path, \"c203.pkl\"), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimensionality of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T18:42:20.194975Z",
     "start_time": "2019-03-12T18:42:19.573815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sol1.time_cube[i][0][7] for i in range(len(sol1.time_cube))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the time (compared to pirmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T19:58:36.536187Z",
     "start_time": "2019-03-12T19:58:36.530184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0812030075187633"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol1.time_cube[0][0][7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
